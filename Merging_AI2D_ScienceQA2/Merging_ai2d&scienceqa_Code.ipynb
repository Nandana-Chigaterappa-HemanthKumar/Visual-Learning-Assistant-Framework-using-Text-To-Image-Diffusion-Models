{"cells":[{"cell_type":"markdown","id":"dfa2b1d3-68e0-449f-8d76-980f2342a7f2","metadata":{"id":"dfa2b1d3-68e0-449f-8d76-980f2342a7f2"},"source":["### Adding domain field in both ai2d and science qa"]},{"cell_type":"code","execution_count":null,"id":"16b02ea2-8c26-404e-a1fc-f25852fc3e11","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16b02ea2-8c26-404e-a1fc-f25852fc3e11","executionInfo":{"status":"ok","timestamp":1745446902404,"user_tz":420,"elapsed":134,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"3e368ab8-c0ac-40e4-d157-8b1a298a16f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1 complete: JSON files saved.\n"]}],"source":["import json\n","import os\n","\n","# File paths\n","ai2d_in = \"ai2d/ai2d_educational_captions_filtered.json\"  # .json format\n","scienceqa_in = \"scienceqa/llava_image_captions_only.jsonl\"     # .jsonl format\n","\n","ai2d_out = \"ai2d_labeled.json\"\n","scienceqa_out = \"scienceqa_labeled.json\"\n","\n","# Processing AI2D JSON\n","ai2d_result = []\n","with open(ai2d_in, \"r\") as f_in:\n","    ai2d_data = json.load(f_in)\n","    for item in ai2d_data:\n","        if item[\"caption\"].strip():\n","            ai2d_result.append({\n","                \"image_path\": os.path.join(\"ai2d\", item[\"image\"]),\n","                \"caption\": item[\"caption\"].strip(),\n","                \"domain\": \"AI2D\"\n","            })\n","\n","with open(ai2d_out, \"w\") as f_out:\n","    json.dump(ai2d_result, f_out, indent=2)\n","\n","# Processing ScienceQA JSONL\n","scienceqa_result = []\n","with open(scienceqa_in, \"r\") as f_in:\n","    for line in f_in:\n","        item = json.loads(line)\n","        if \"llava_caption\" in item and item[\"llava_caption\"].strip():\n","            scienceqa_result.append({\n","                \"image_path\": os.path.join(\"scienceqa/images\", f\"{item['image_id']}.png\"),\n","                \"caption\": item[\"llava_caption\"].strip(),\n","                \"domain\": \"ScienceQA\"\n","            })\n","\n","with open(scienceqa_out, \"w\") as f_out:\n","    json.dump(scienceqa_result, f_out, indent=2)\n","\n","print(\"Step 1 complete: JSON files saved.\")"]},{"cell_type":"markdown","id":"2cdd4a7b-5841-42a1-a33a-923f0e1fecc2","metadata":{"id":"2cdd4a7b-5841-42a1-a33a-923f0e1fecc2"},"source":["### Combining ai2d and science qa"]},{"cell_type":"code","execution_count":null,"id":"43b1d626-8982-4357-b489-40d479c5946b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43b1d626-8982-4357-b489-40d479c5946b","executionInfo":{"status":"ok","timestamp":1745446939798,"user_tz":420,"elapsed":107,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"83c01a51-07b9-45b3-a510-4efe68b50376"},"outputs":[{"output_type":"stream","name":"stdout","text":["Combined dataset saved to: combined_dataset.jsonl\n"]}],"source":["import json\n","\n","# Input files (from Step 1)\n","ai2d_file = \"ai2d_labeled.json\"\n","scienceqa_file = \"scienceqa_labeled.json\"\n","output_file = \"combined_dataset.jsonl\"\n","\n","# Loading both the JSON lists\n","with open(ai2d_file, \"r\") as f:\n","    ai2d_data = json.load(f)\n","\n","with open(scienceqa_file, \"r\") as f:\n","    scienceqa_data = json.load(f)\n","\n","# Combining the two lists\n","combined_data = ai2d_data + scienceqa_data\n","\n","# Writing to JSONL file\n","with open(output_file, \"w\") as f_out:\n","    for entry in combined_data:\n","        f_out.write(json.dumps(entry) + \"\\n\")\n","\n","print(f\"Combined dataset saved to: {output_file}\")"]},{"cell_type":"markdown","id":"f0d6cbee-abc7-4bb4-bc82-9171d986d6cf","metadata":{"id":"f0d6cbee-abc7-4bb4-bc82-9171d986d6cf"},"source":["###  Defining the Domain Map\n"]},{"cell_type":"code","execution_count":null,"id":"30e96e78-2535-449c-8372-346ccb1c2235","metadata":{"id":"30e96e78-2535-449c-8372-346ccb1c2235"},"outputs":[],"source":["# Mapping domains to numerical labels\n","domain_dict = {\n","    \"AI2D\": 0,\n","    \"ScienceQA\": 1\n","}"]},{"cell_type":"markdown","id":"5bceca0e-ad28-4268-a488-573086108d5d","metadata":{"id":"5bceca0e-ad28-4268-a488-573086108d5d"},"source":["### Defining Image Transformations (PyTorch)"]},{"cell_type":"markdown","source":["This sets a standard transformation for all your images:\n","\t•\tResize all to 256x256 (matches VAE input)\n","\t•\tConvert image to PyTorch Tensor (scales pixel values from 0–255 → 0–1)\n"],"metadata":{"id":"9y1H6MoXUq8K"},"id":"9y1H6MoXUq8K"},{"cell_type":"code","execution_count":null,"id":"4089235d-2976-4229-b691-203468d28c22","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4089235d-2976-4229-b691-203468d28c22","executionInfo":{"status":"ok","timestamp":1745446980218,"user_tz":420,"elapsed":1826,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"9d77412f-faa6-4664-d28a-8ee5b1da7ddd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from torchvision import transforms\n","\n","# Standard image preprocessing\n","image_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # Resizing to 256x256\n","    transforms.ToTensor(),          # Converting to tensor and normalizing to [0, 1]\n","])"]},{"cell_type":"markdown","id":"731700d3-370c-40af-82aa-ff778fe1ec91","metadata":{"id":"731700d3-370c-40af-82aa-ff778fe1ec91"},"source":["###  Implementing Text Tokenizer and Embedder"]},{"cell_type":"code","execution_count":null,"id":"30002509-4340-4a47-93e4-9ec8b85fbc45","metadata":{"id":"30002509-4340-4a47-93e4-9ec8b85fbc45"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["\t•\tLoads the pretrained tokenizer and text encoder.\n","\t•\ttokenizer turns your caption into token IDs.\n","\t•\ttext_encoder converts token IDs into embeddings."],"metadata":{"id":"en_o8m1gUma0"},"id":"en_o8m1gUma0"},{"cell_type":"code","execution_count":null,"id":"5560d96b-88e3-4804-8fb4-3f3d6351a2bd","metadata":{"id":"5560d96b-88e3-4804-8fb4-3f3d6351a2bd"},"outputs":[],"source":["#Tokenizer Initialization\n","\n","from transformers import CLIPTokenizer, CLIPTextModel\n","\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"]},{"cell_type":"code","execution_count":null,"id":"7a779c84-444c-4dd9-a615-013f8b2010c0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7a779c84-444c-4dd9-a615-013f8b2010c0","executionInfo":{"status":"ok","timestamp":1745447005193,"user_tz":420,"elapsed":685,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"ba92979c-7d5e-40a7-d53e-a00363490c90"},"outputs":[{"output_type":"stream","name":"stderr","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["#Text Encoder Initialization\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")"]},{"cell_type":"markdown","source":["\t•\tInput: text – a single string (caption like “Photosynthesis diagram”).\n","\t•\tStep 1: Tokenizes it and turns it into tensors (input_ids, attention_mask).\n","\t•\tStep 2: Passes it through the CLIP text encoder.\n","\t•\tStep 3: Takes the mean over the token embeddings to get a single [768]-dim vector.\n","\t•\tReturns: A vector that represents the text meaning in CLIP’s latent space."],"metadata":{"id":"18FhAFzNUhkI"},"id":"18FhAFzNUhkI"},{"cell_type":"code","execution_count":null,"id":"193c5ebb-2ad7-4db9-beda-cfb2b30c2b87","metadata":{"id":"193c5ebb-2ad7-4db9-beda-cfb2b30c2b87"},"outputs":[],"source":["## Embedding Function\n","def embed_text(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n","    with torch.no_grad():\n","        embedding = text_encoder(**inputs).last_hidden_state.mean(dim=1)\n","    return embedding.squeeze()"]},{"cell_type":"markdown","id":"9dd22d52-9025-42ed-a3b9-4c587dc09fa4","metadata":{"id":"9dd22d52-9025-42ed-a3b9-4c587dc09fa4"},"source":["### Unified Dataset Loader"]},{"cell_type":"markdown","source":["You are creating a custom PyTorch Dataset class to:\n","\t1.\tLoad and transform educational diagram images.\n","\t2.\tConvert captions into CLIP embeddings.\n","\t3.\tEncode the data source (AI2D or ScienceQA) for multi-domain handling."],"metadata":{"id":"GspNJH58VAOV"},"id":"GspNJH58VAOV"},{"cell_type":"code","execution_count":null,"id":"ffdfd3f0-4197-40dc-bf06-67d87e3e6c43","metadata":{"id":"ffdfd3f0-4197-40dc-bf06-67d87e3e6c43"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import json\n","import torch\n","import os\n","\n","class EducationalDiagramDataset(Dataset):\n","    def __init__(self, metadata_path, text_embedder, domain_dict, transform):\n","        with open(metadata_path, \"r\") as f:\n","            samples = [json.loads(line) for line in f]\n","\n","        self.samples = []\n","        self.skipped_count = 0\n","        self.text_embedder = text_embedder\n","        self.domain_dict = domain_dict\n","        self.transform = transform\n","\n","        # Filtering out missing image files during initialization\n","        for sample in samples:\n","            if os.path.exists(sample[\"image_path\"]):\n","                self.samples.append(sample)\n","            else:\n","                self.skipped_count += 1\n","\n","        print(f\"Loaded {len(self.samples)} valid samples\")\n","        print(f\"Skipped {self.skipped_count} missing or invalid image files\")\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","\n","        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n","        image = self.transform(image)\n","\n","        caption = sample[\"caption\"]\n","        text_embedding = self.text_embedder(caption)\n","\n","        domain_idx = self.domain_dict[sample[\"domain\"]]\n","\n","        return image, text_embedding, domain_idx\n","\n","    def __len__(self):\n","        return len(self.samples)"]},{"cell_type":"code","execution_count":null,"id":"41d432a1-9095-4638-a4a1-edd33d5160ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41d432a1-9095-4638-a4a1-edd33d5160ef","executionInfo":{"status":"ok","timestamp":1745447079411,"user_tz":420,"elapsed":12460,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"95e84da8-0d55-442a-f9c8-2effc2647e32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 9573 valid samples\n","Skipped 468 missing or invalid image files\n","torch.Size([8, 3, 256, 256])\n","torch.Size([8, 512])\n","tensor([1, 1, 0, 1, 0, 1, 1, 0])\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","dataset = EducationalDiagramDataset(\n","    metadata_path=\"combined_dataset.jsonl\",\n","    text_embedder=embed_text,\n","    domain_dict={\"AI2D\": 0, \"ScienceQA\": 1},\n","    transform=image_transform\n",")\n","\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","for images, text_embeddings, domain_labels in dataloader:\n","    print(images.shape)          # [8, 3, 256, 256]\n","    print(text_embeddings.shape) # [8, 768]\n","    print(domain_labels)         # tensor([0, 1, 0, ...])\n","    break"]},{"cell_type":"markdown","id":"9e6fe60d-00c3-4c13-a069-643dd093293f","metadata":{"id":"9e6fe60d-00c3-4c13-a069-643dd093293f"},"source":["### Saving the file"]},{"cell_type":"markdown","source":["You are converting your full dataset into 3 tensors:\n","\t1.\tImages [N, 3, 256, 256]\n","\t2.\tText Embeddings [N, 768] (from CLIP)\n","\t3.\tLabels [N] (0 for AI2D, 1 for ScienceQA)\n","\n","Then, you’re saving it as a .pt file (educational_diagram_data.pt) — this makes it quick to load later during VAE or LDM training without recomputing embeddings."],"metadata":{"id":"Yhi_3Yx2VTt_"},"id":"Yhi_3Yx2VTt_"},{"cell_type":"code","execution_count":null,"id":"b5658027-d8cb-4fe4-b215-13fa3f7891bc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5658027-d8cb-4fe4-b215-13fa3f7891bc","executionInfo":{"status":"ok","timestamp":1745447506577,"user_tz":420,"elapsed":372716,"user":{"displayName":"Nandana Chigaterappa Hemanthkumar","userId":"09739011327818673956"}},"outputId":"b8ea2a0b-0159-4a9a-b198-e8a9427b1ad6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 9573 valid samples\n","Skipped 468 missing or invalid image files\n","Saving embeddings...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9573/9573 [05:31<00:00, 28.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved all tensors to 'educational_diagram_data.pt'\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","dataset = EducationalDiagramDataset(\n","    metadata_path=\"combined_dataset.jsonl\",\n","    text_embedder=embed_text,\n","    domain_dict={\"AI2D\": 0, \"ScienceQA\": 1},\n","    transform=image_transform\n",")\n","\n","dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n","\n","# Collecting all into lists\n","all_images = []\n","all_text_embeddings = []\n","all_domain_labels = []\n","\n","print(\"Saving embeddings...\")\n","\n","for image, text_emb, domain in tqdm(dataloader):\n","    all_images.append(image.squeeze(0))           # shape: [3, 256, 256]\n","    all_text_embeddings.append(text_emb.squeeze(0))  # shape: [512]\n","    all_domain_labels.append(domain.item())       # single int\n","\n","# Converting to tensors\n","all_images = torch.stack(all_images)\n","all_text_embeddings = torch.stack(all_text_embeddings)\n","all_domain_labels = torch.tensor(all_domain_labels)\n","\n","# Saving to .pt file\n","torch.save({\n","    \"images\": all_images,\n","    \"text_embeddings\": all_text_embeddings,\n","    \"labels\": all_domain_labels\n","}, \"educational_diagram_data.pt\")\n","\n","print(\"Saved all tensors to 'educational_diagram_data.pt'\")"]},{"cell_type":"code","execution_count":null,"id":"bd8dc843-cd49-492d-aaba-0bdfeea12556","metadata":{"id":"bd8dc843-cd49-492d-aaba-0bdfeea12556"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}